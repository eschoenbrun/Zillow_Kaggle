{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path_to_data='data', sample_size = None):\n",
    "    logging.info('Reading Properties 2016...')\n",
    "    prop_2016 = pd.read_csv('{}/properties_2016.csv'.format(path_to_data))\n",
    "    \n",
    "    logging.info('Reading Properties 2017...')\n",
    "    prop_2017 = pd.read_csv('{}/properties_2017.csv'.format(path_to_data))\n",
    "    \n",
    "    logging.info('Reading Train 2016...')\n",
    "    target_2016 = pd.read_csv('{}/train_2016_v2.csv'.format(path_to_data))\n",
    "    \n",
    "    logging.info('Reading Train 2017..')\n",
    "    target_2017 = pd.read_csv('{}/train_2017.csv'.format(path_to_data))\n",
    "    \n",
    "    logging.info('Performing merge')\n",
    "    joined_data_2016 = pd.merge(target_2016,prop_2016,on=\"parcelid\",how=\"left\")\n",
    "    joined_data_2017 = pd.merge(target_2017, prop_2017,on='parcelid',how='left')\n",
    "\n",
    "    joined_data = pd.concat([joined_data_2016,joined_data_2017])\n",
    "\n",
    "    # convert dates:\n",
    "    joined_data.transactiondate = pd.to_datetime(joined_data.transactiondate,format=\"%Y-%m-%d\")\n",
    "\n",
    "    joined_data['transaction_mth'] = joined_data.transactiondate.apply(lambda x:x.month)\n",
    "    joined_data['transaction_yr'] = joined_data.transactiondate.apply(lambda x: x.year)\n",
    "    joined_data['transaction_day_of_wk'] = joined_data.transactiondate.apply(lambda x: x.dayofweek)\n",
    "    joined_data=joined_data.drop('transactiondate',axis=1)\n",
    "    \n",
    "    del target_2016\n",
    "    del target_2017\n",
    "    del prop_2016\n",
    "    del prop_2017\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "#    if sample_size is not None:\n",
    "#        logging.info('Sampling: {} of data'.format(sample_size))\n",
    "#        joined_data = joined_data.sample(frac=sample_size)\n",
    "\n",
    "    return joined_data, joined_data['logerror'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_data = pd.read_csv('C:/Users/Stevens/Desktop/BIA 686/zillow/bia_686/data/CA_zip_city.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(joined_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_missing(joined_data):\n",
    "    ## calculatedbathnbr same as bathroomcnt and has more nas\n",
    "    joined_data[joined_data.bathroomcnt !=joined_data.calculatedbathnbr][['bathroomcnt', 'calculatedbathnbr']].dropna()\n",
    "    joined_data[['bathroomcnt', 'calculatedbathnbr']].isnull().sum()\n",
    "\n",
    "    #Fill in those properties that have a pool with median pool value\n",
    "    poolsizesum_median = joined_data.loc[joined_data['poolsizesum'] > 0, 'poolsizesum'].median()\n",
    "    joined_data.loc[(joined_data['poolsizesum'].isnull() | \n",
    "                    joined_data['poolsizesum'] == 0) & \n",
    "                ((pd.notnull(joined_data['poolcnt'])) |\n",
    "                (pd.notnull(joined_data['pooltypeid10'])) |\n",
    "                (pd.notnull(joined_data['pooltypeid2'])) |\n",
    "                (pd.notnull(joined_data['pooltypeid7']))), 'poolsizesum'] = poolsizesum_median\n",
    "\n",
    "    #Fill in those properties that have a pool size/type with pool count of 1\n",
    "    joined_data.loc[(joined_data['poolcnt'] != 1) & \n",
    "                ((pd.notnull(joined_data['poolsizesum'])) |\n",
    "                (pd.notnull(joined_data['pooltypeid10'])) |\n",
    "                (pd.notnull(joined_data['pooltypeid2'])) |\n",
    "                (pd.notnull(joined_data['pooltypeid7']))), 'poolcnt'] = 1\n",
    "\n",
    "    # fill in fireplace count with mode when there is a fireplace flag is true\n",
    "    joined_data.loc[(joined_data.fireplaceflag == True) & \n",
    "                    (joined_data.fireplacecnt.isnull()), 'fireplacecnt'] = joined_data.fireplacecnt.dropna().mode()[0]\n",
    "\n",
    "    # number of car garage is more that sqft\n",
    "    garage_median = joined_data[(joined_data.garagetotalsqft != 0)].garagetotalsqft.dropna().median()\n",
    "    joined_data.loc[(joined_data.garagetotalsqft == 0) & (joined_data.garagecarcnt > 0), 'garagetotalsqft'] = garage_median\n",
    "\n",
    "    # unitcnt extreme outliers\n",
    "    unit_median = joined_data.loc[pd.notnull(joined_data['unitcnt'])]['unitcnt'].median()\n",
    "    joined_data.loc[(joined_data['unitcnt'] > 9), 'unitcnt'] = unit_median\n",
    "\n",
    "    return joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_columns(data, drop_cols):\n",
    "    # mostly null\n",
    "    data = data.drop(drop_cols, axis=1)\n",
    "    data.drop_duplicates(inplace = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def columns_after_drop(numeric, categorical, drop_columns):\n",
    "    numeric = list(set(numeric) - (set(numeric) & set(drop_columns)))\n",
    "    categorical = list(set(categorical) - (set(categorical) & set(drop_columns)))\n",
    "\n",
    "    return numeric, categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute_numerical_var(joined_data, numerical_cols, imputation= None):\n",
    "    logging.info('Filling numeric NAs')\n",
    "    if imputation:\n",
    "        for col, val in imputations_numeric.items():\n",
    "            if col in properties.columns:\n",
    "                properties[col].fillna(val, inplace=True)\n",
    "                return properties\n",
    "    else:\n",
    "    # numerical vars\n",
    "        numerical_data = joined_data.copy().reset_index()\n",
    "        numerical_data = numerical_data[numerical_cols]\n",
    "        numerical_data_cols = numerical_data.columns\n",
    "\n",
    "        numeric_imp  = Imputer(strategy='median', axis=0)     \n",
    "        numerical_data = pd.DataFrame(numeric_imp.fit_transform(numerical_data.values), columns=numerical_data_cols)\n",
    "\n",
    "        return numerical_data,  {key:val for key,val in  zip(numerical_data_cols, numeric_imp.statistics_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impute_categorical_var(joined_data, categorical_cols):\n",
    "    # categorical vars\n",
    "    categorical_data = joined_data.copy().reset_index()\n",
    "    categorical_data  = categorical_data[categorical_cols]\n",
    "\n",
    "    if 'hashottuborspa' in categorical_cols:\n",
    "        categorical_data['hashottuborspa']=categorical_data['hashottuborspa'].apply(lambda x: 1 if x == 'True' else 0)\n",
    "\n",
    "    if 'taxdelinquencyflag' in categorical_cols:\n",
    "        categorical_data['taxdelinquencyflag']=categorical_data['taxdelinquencyflag'].apply(lambda x: 1 if str(x).strip().lower() == 'y' else 0)\n",
    "\n",
    "    for c, dtype in zip(categorical_data.columns, categorical_data.dtypes):\n",
    "        categorical_data[c] = categorical_data[c].apply(lambda x: x if pd.isnull(x) else str(x))\n",
    "\n",
    "    categorical_data_cols = categorical_data.columns\n",
    "\n",
    "    most_frequent_lst = []\n",
    "    \n",
    "    logging.info('Using most frequent...')\n",
    "    \n",
    "    for col in categorical_data_cols:\n",
    "        logging.info(\"Filling NA: {}\".format(col))\n",
    "        # logging.info(\"Filling NA: {}\".format(col))\n",
    "        mk=categorical_data[col].notnull()\n",
    "        value_counts = categorical_data[mk][col].value_counts()\n",
    "        most_frequent_lst.append(value_counts.index[0])\n",
    "        categorical_data[col].fillna(most_frequent_lst[-1], inplace=True)\n",
    "\n",
    "    return categorical_data, {key:val for key,val in zip(categorical_data_cols, most_frequent_lst)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = ['parcelid', 'assessmentyear','basementsqft',\t'bathroomcnt',\t'bedroomcnt',\t'calculatedbathnbr', \n",
    "                'calculatedfinishedsquarefeet',\t'finishedfloor1squarefeet',\t'finishedsquarefeet12',\n",
    "                'finishedsquarefeet13',\t'finishedsquarefeet15',\t'finishedsquarefeet50',\t'finishedsquarefeet6',\n",
    "                'fireplacecnt',\t'fullbathcnt',\t'garagecarcnt',\t'garagetotalsqft',\t'landtaxvaluedollarcnt',\n",
    "                'lotsizesquarefeet',\t'numberofstories',\t'poolcnt',\t'poolsizesum',\t'roomcnt',\n",
    "                'structuretaxvaluedollarcnt',\t'taxamount',\t'taxvaluedollarcnt',\t'threequarterbathnbr',\n",
    "                'unitcnt',\t'yardbuildingsqft17',\t'yardbuildingsqft26','transaction_day_of_wk','transaction_mth','transaction_yr',\n",
    "               'taxdelinquencyyear','yearbuilt','latitude','longitude']\n",
    "\n",
    "categorical_cols = ['parcelid', 'airconditioningtypeid','architecturalstyletypeid','buildingclasstypeid','buildingqualitytypeid',\n",
    "                'censustractandblock','decktypeid','fips','fireplaceflag','hashottuborspa',\n",
    "                 'heatingorsystemtypeid','pooltypeid10','pooltypeid2','pooltypeid7','propertycountylandusecode',\n",
    "                 'propertylandusetypeid','propertyzoningdesc', 'rawcensustractandblock','regionidcity','regionidcounty','regionidneighborhood','regionidzip',\n",
    "                 'storytypeid','taxdelinquencyflag','typeconstructiontypeid']\n",
    "\n",
    "drop_cols = ['buildingclasstypeid','propertyzoningdesc',\t'numberofstories',\t'threequarterbathnbr',\t\n",
    "            'finishedfloor1squarefeet','finishedsquarefeet50','finishedsquarefeet15',\n",
    "            'finishedsquarefeet12', 'yardbuildingsqft17',\t'finishedsquarefeet6',\t'yardbuildingsqft26',\t\n",
    "            'basementsqft',\t'finishedsquarefeet13','assessmentyear','calculatedbathnbr',\n",
    "            'rawcensustractandblock', 'regionidzip','regionidcounty','regionidcity','regionidneighborhood',\n",
    "            'regionidneighborhood','taxvaluedollarcnt','buildingclasstypeid','storytypeid']\t\n",
    "\n",
    "\n",
    "# dropped columns removed: \n",
    "# 'garagetotalsqft',\t'garagecarcnt',\t'poolcnt','fireplacecnt',\t'poolsizesum', 'censustractandblock',\n",
    "# 'fireplaceflag',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use pipeline\n",
    "joined_data, logerror_var = load_data(path_to_data='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(joined_data, 'C:/Users/Stevens/Desktop/BIA 686/zillow/bia_686/data/joined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use pipeline\n",
    "joined_data, logerror_var = load_data(path_to_data='data')\n",
    "# joined_data = drop_columns(joined_data, drop_cols)\n",
    "joined_data, NAlist = reduce_mem_usage(joined_data)\n",
    "joined_data = calculate_missing(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you need to drop a column, add it to drop_cols\n",
    "# numeric_cols, categorical_cols = columns_after_drop(numeric_cols, categorical_cols, drop_cols)\n",
    "numeric_data, imputations_numeric = impute_numerical_var(joined_data, numeric_cols)\n",
    "categorical_data, imputations_categorical = impute_categorical_var(joined_data, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.merge(numeric_data, categorical_data, on=\"parcelid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cat_data = pd.merge(numeric_data, categorical_data, on=\"parcelid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_data = pd.merge(num_cat_data, joined_data[['parcelid', 'logerror']],on='parcelid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cols = list(set(numeric_cols) - set(drop_cols))\n",
    "train, test = train_test_split(joined_data, test_size=0.2,random_state=4)\n",
    "\n",
    "train = train.sample(frac=0.01)\n",
    "X_train = train[num_cols].drop('parcelid',axis=1)\n",
    "y_train = train['logerror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_feat = RandomForestRegressor(n_jobs=-1,criterion='mae', random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rf_feat.fit(X_train,y_train)\n",
    "stop = time.time()\n",
    "print(\"Elapsed_time: {}\".format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When we use 1,343 observations\n",
    "for numeric_feature, score in sorted(zip(rf_feat.feature_importances_,X_train.columns),reverse=True):\n",
    "    print(numeric_feature, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = categorical_cols[1:]\n",
    "cat_train = train[categorical_cols].copy()\n",
    "cat_dummies = pd.get_dummies(cat_train[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_train[categorical_cols].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_feat.fit(cat_dummies, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When we use 13,431 observations\n",
    "for numeric_feature, score in sorted(zip(rf_feat.feature_importances_,cat_dummies.columns),reverse=True):\n",
    "    print(numeric_feature, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
